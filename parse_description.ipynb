{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Safari\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJobInfo(url, driver):\n",
    "    \n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        jobTitle = soup.title.text.split(' | ')[0]\n",
    "    except AttributeError:\n",
    "        return None\n",
    "    jobID = int(soup.find('code', {\"id\":'decoratedJobPostingId'}).string[1:-1])\n",
    "    \n",
    "    company = jobTitle.split(' hiring ')[0].replace('\"', '')\n",
    "    potision = jobTitle.split(' hiring ')[1].split(' in ')[0].replace('\"', '')\n",
    "    location = jobTitle.split(' hiring ')[1].split(' in ')[1].replace('\"', '')\n",
    "    try:\n",
    "        location = location.split(', ')[2]\n",
    "    except:\n",
    "        try:\n",
    "            location = location.split(', ')[1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # TODO: need to find a better way to get requirements\n",
    "    keywords = [\"require\", \"qualifi\", \"skill\", \"have\", \"looking for\", \"must be\", \"ideal\", \"succ\"]\n",
    "    description = soup.find('div', {\"class\":'description__text description__text--rich'})\n",
    "    for br in description.find_all(\"br\"):\n",
    "        br.extract()\n",
    "    lists = description.find_all(\"ul\")\n",
    "\n",
    "    requirements = \"\"\n",
    "    for ulist in lists:\n",
    "        try:\n",
    "            if any(keyword in ulist.previous_sibling.text.lower() for keyword in keywords):\n",
    "                requirements = [requirement.text for requirement in ulist.find_all('li')]\n",
    "                requirements = \" \".join(requirements).replace('\\n', ' ').replace(';', ',')\n",
    "            elif any(keyword in ulist.previous_sibling.previous_sibling.find('strong').text.lower() for keyword in keywords):\n",
    "                requirements = [requirement.text for requirement in ulist.find_all('li')]\n",
    "                requirements = \" \".join(requirements).replace('\\n', ' ')\n",
    "            elif any(keyword in ulist.previous_sibling.previous_sibling.find('p').text.lower() for keyword in keywords):\n",
    "                requirements = [requirement.text for requirement in ulist.find_all('li')]\n",
    "                requirements = \" \".join(requirements).replace('\\n', ' ')\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    row = [jobID, company, potision, location, requirements, url]\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed job: Orbis Junior Data Engineer\n",
      "parsed job: European Recruitment Graduate Software Developer - STEM / Physics / Mathematics\n",
      "parsed job: SoundHound AI UPCOMING: Software Engineer, Conversational AI\n",
      "parsed job: Microsoft Data Scientist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveJobsCsvFromUrls(txtfile='urls.txt', urlList=None, csvfile='jobs.csv'):\n",
    "    \n",
    "    if urlList == None:\n",
    "        f = open(txtfile, 'r')\n",
    "        urlList = f.readlines()\n",
    "        urlList = [x.strip() for x in urlList]\n",
    "        f.close()\n",
    "\n",
    "    try:\n",
    "        oldJobs = pd.read_csv(csvfile, sep=';')\n",
    "    except:\n",
    "        oldJobs = pd.DataFrame(columns=['JobID', 'Company', 'Position', 'Location', 'Requirements', 'JobUrl', 'Interview Status', 'Offer Status'])\n",
    "    \n",
    "    driver = webdriver.Safari()\n",
    "    rows = []\n",
    "    failedJobs = 0\n",
    "    for url in urlList:\n",
    "        jobid = int(url.split('/')[-2])\n",
    "    \n",
    "        if jobid in list(oldJobs['JobID']):\n",
    "            continue\n",
    "        # sometimes the web driver gets stuck when retrieving the page. if that's the case, we eill retry 5 times\n",
    "        counter = 0\n",
    "        while counter < 5:\n",
    "            row = getJobInfo(url, driver)\n",
    "            if row is not None:\n",
    "                print(\"parsed job: \" + row[1] + \" \" + row[2])\n",
    "                rows.append(row)\n",
    "                break\n",
    "            counter += 1\n",
    "        \n",
    "        if counter == 5:\n",
    "            print(\"failed to parse job: \" + url)\n",
    "            failedJobs += 1\n",
    "            \n",
    "    driver.quit()\n",
    "\n",
    "    jobs = pd.DataFrame(rows, columns=['JobID', 'Company', 'Position', 'Location', 'Requirements', 'JobUrl'])\n",
    "\n",
    "    jobs['Interview Status'] = 'Pending'\n",
    "    jobs['Offer Status'] = 'Pending'\n",
    "\n",
    "\n",
    "#add non-duplicate rows to oldJobs\n",
    "    newJobs = jobs[~jobs['JobID'].isin(oldJobs['JobID'])]\n",
    "    oldJobs = pd.concat([oldJobs, newJobs])\n",
    "    oldJobs.to_csv(csvfile, index=False, sep=';')\n",
    "    return failedJobs\n",
    "\n",
    "saveJobsCsvFromUrls(csvfile='jobs4.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
